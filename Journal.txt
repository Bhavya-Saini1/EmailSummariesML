JOURNAL:

Description: Full documentation of process for creating project including challenges faced, websites referenced, etc.
Legend: -> are steps take, - are ideas/thought processes

PHASE 1: PLANNING

Reviewed Documentation principles: https://github.com/resources/articles/tools-and-techniques-for-effective-code-documentation

Goal: Use Classic ML for Email Summary

- Most Summarizers found in online research use a LLM API, not relevant to project goal of ground-up NLP
    e.g. https://builder.aws.com/content/36L4zbM3nct3QotI2zbXoCob42e/automating-email-overload-how-i-built-an-ai-email-summarizer-in-one-afternoon
- I could test word frequency to find important sentences, but that wouldn't use ML
- Project goal does not fit neatly into Supervised Learning or Unsupervsied Learning as it is a generative problem not classificaiton based
- Must create some type of generative model

Queried with Gemini: https://gemini.google.com/share/a2bb40ddc653

- Using a Deep Learning (DL) Model, implementing Recurrent Neural Network (RNN) is a good approach for my scope

Revised Goal: Use Sequential to Sequential (Seq2Seq) DL Model for Email Summary

Next Steps: Setup enviornment with PyTorch to begin next steps of Data Collection & Preparation


PHASE 2: SETUP

Goal: Create files and classes, make imports and add libraries.

- Specific to a Seq2Seq Long Short-Term Memory (LSTM) where LSTM allows for the use of gates to decide which data is forgotten and rememebered
- This type of model will require a Encoder RNN to create context values
- Additionally, a Decoder RNN to generate summary based off context
- and then an app interface to do both at once

Reviewed: https://docs.pytorch.org/docs/stable/index.html
Read: https://medium.com/@doublekien/lstm-with-pytorch-664065718df4

Concept: Teacher Forcing - If the model guesses the wrong word, the whole sentence
may end up incorrect. Instead, give the model the correct word in the next step despite prediction.

-> Made encoder.py file
-> Made decoder.py file
-> Made Seq2Seq.py file

- Need to be able to tokenize the text in an email for the data to be processed by the model
- this should be done in a seperate lexicon or vocab class

-> Made Vocabulary.py file

- Need a data reader/storer class for this

-> Made Dataset Class

- Been struggling with remembering all the pytorch conventions so im doing plenty of comments
- From what I've seen I'll need to pad out my data to have emails be of consistent length
- use pytorch collate.fn

-> Made Collate Class
-> Make a main file to run code and train model

- Realized to document errors too

ERROR:
Traceback (most recent call last):
  File "/Users/bhavyasaini/PycharmProjects/EmailSummariesML/training.py", line 43, in <module>
    vocab = Vocabulary(freq_threshold=1)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/bhavyasaini/PycharmProjects/EmailSummariesML/Vocabulary.py", line 23, in __init__
    self.spacy_eng = spacy.load("en_core_web_sm")
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/bhavyasaini/PycharmProjects/CSC108 Assignment 2/.venv/lib/python3.11/site-packages/spacy/__init__.py", line 52, in load
    return util.load_model(
           ^^^^^^^^^^^^^^^^
  File "/Users/bhavyasaini/PycharmProjects/CSC108 Assignment 2/.venv/lib/python3.11/site-packages/spacy/util.py", line 531, in load_model
    raise IOError(Errors.E050.format(name=name))
OSError: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory.

- I need to install spacy via terminal but PyCharm terminal wont let me, I'll just do it in the file

ERROR:
Traceback (most recent call last):
  File "/Users/bhavyasaini/PycharmProjects/EmailSummariesML/training.py", line 87, in <module>
    output = model(src, trg)
             ^^^^^^^^^^^^^^^
  File "/Users/bhavyasaini/PycharmProjects/CSC108 Assignment 2/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/bhavyasaini/PycharmProjects/CSC108 Assignment 2/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/bhavyasaini/PycharmProjects/EmailSummariesML/Seq2Seq.py", line 47, in forward
    output, hidden, cell = self.decoder(input, hidden, cell)
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/bhavyasaini/PycharmProjects/CSC108 Assignment 2/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/bhavyasaini/PycharmProjects/CSC108 Assignment 2/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/bhavyasaini/PycharmProjects/EmailSummariesML/Decoder.py", line 43, in forward
    embedded = self.dropout(self.embedding_size(input))
                            ^^^^^^^^^^^^^^^^^^^
  File "/Users/bhavyasaini/PycharmProjects/CSC108 Assignment 2/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1964, in __getattr__
    raise AttributeError(
AttributeError: 'Decoder' object has no attribute 'embedding_size'

- Simple naming error when i was writing the Decoder

ERROR:
Traceback (most recent call last):
  File "/Users/bhavyasaini/PycharmProjects/EmailSummariesML/training.py", line 87, in <module>
    output = model(src, trg)
             ^^^^^^^^^^^^^^^
  File "/Users/bhavyasaini/PycharmProjects/CSC108 Assignment 2/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/bhavyasaini/PycharmProjects/CSC108 Assignment 2/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/bhavyasaini/PycharmProjects/EmailSummariesML/Seq2Seq.py", line 47, in forward
    output, hidden, cell = self.decoder(input, hidden, cell)
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/bhavyasaini/PycharmProjects/CSC108 Assignment 2/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/bhavyasaini/PycharmProjects/CSC108 Assignment 2/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/bhavyasaini/PycharmProjects/EmailSummariesML/Decoder.py", line 46, in forward
    output, (hidden, cell) = self.rnn(embedded, (hidden, cell))
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/bhavyasaini/PycharmProjects/CSC108 Assignment 2/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/bhavyasaini/PycharmProjects/CSC108 Assignment 2/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/bhavyasaini/PycharmProjects/CSC108 Assignment 2/.venv/lib/python3.11/site-packages/torch/nn/modules/rnn.py", line 1078, in forward
    raise ValueError(
ValueError: LSTM: Expected input to be 2D or 3D, got 4D instead

- I'm adding to many dimensions I'll just get rid of the extra one in the decoder

MODEL TRAINING:

Epoch: 10 | Loss: 1.3080
Epoch: 20 | Loss: 0.1624
Epoch: 30 | Loss: 0.0324
Epoch: 40 | Loss: 0.0088
Epoch: 50 | Loss: 0.0038
Epoch: 60 | Loss: 0.0027
Epoch: 70 | Loss: 0.0017
Epoch: 80 | Loss: 0.0015
Epoch: 90 | Loss: 0.0012
Epoch: 100 | Loss: 0.0011
Training Done

- Got my model working for the first time with the dummy dataset