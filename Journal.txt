JOURNAL:

Description: Full documentation of process for creating project including challenges faced, websites referenced, etc.
Legend: -> are steps take, - are ideas/thought processes

PHASE 1: PLANNING

Reviewed Documentation principles: https://github.com/resources/articles/tools-and-techniques-for-effective-code-documentation

Goal: Use Classic ML for Email Summary

- Most Summarizers found in online research use a LLM API, not relevant to project goal of ground-up NLP
    e.g. https://builder.aws.com/content/36L4zbM3nct3QotI2zbXoCob42e/automating-email-overload-how-i-built-an-ai-email-summarizer-in-one-afternoon
- I could test word frequency to find important sentences, but that wouldn't use ML
- Project goal does not fit neatly into Supervised Learning or Unsupervsied Learning as it is a generative problem not classificaiton based
- Must create some type of generative model

Queried with Gemini: https://gemini.google.com/share/a2bb40ddc653

- Using a Deep Learning (DL) Model, implementing Recurrent Neural Network (RNN) is a good approach for my scope

Revised Goal: Use Sequential to Sequential (Seq2Seq) DL Model for Email Summary

Next Steps: Setup enviornment with PyTorch to begin next steps of Data Collection & Preparation


PHASE 2: SETUP

Goal: Create files and classes, make imports and add libraries.

- Specific to a Seq2Seq Long Short-Term Memory (LSTM) where LSTM allows for the use of gates to decide which data is forgotten and rememebered
- This type of model will require a Encoder RNN to create context values
- Additionally, a Decoder RNN to generate summary based off context
- and then an app interface to do both at once

Reviewed: https://docs.pytorch.org/docs/stable/index.html
Read: https://medium.com/@doublekien/lstm-with-pytorch-664065718df4

- Made encoder.py file
- Made decoder.py file
- Make Seq2Seq.py file
